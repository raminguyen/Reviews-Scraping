{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6a3d51-025d-4a06-9b35-c0e2b9a2b33b",
   "metadata": {},
   "source": [
    "# Web Scraping Amazon Reviews\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "The goal of this project is to scrape 100 reviews from each review page for two selected travel bag products on Amazon. The process involves extracting essential information from user reviews, including the reviewer's name, review title, star rating, date of the review, and the review content. This data is then compiled into a structured format and saved as a CSV file for further analysis.\n",
    "\n",
    "## Steps Involved\n",
    "\n",
    "### 1. Setting Up the Environment\n",
    "- **Libraries Used**: \n",
    "  - `requests` for making HTTP requests\n",
    "  - `pandas` for data manipulation\n",
    "  - `BeautifulSoup` from `bs4` for parsing HTML\n",
    "  - `datetime` for handling date formatting\n",
    "  - `time` and `random` for adding delays between requests to mimic human browsing behavior\n",
    "- **Logging Configuration**: \n",
    "  - Configured logging to capture any errors that occur during the scraping process.\n",
    "\n",
    "### 2. Headers Configuration\n",
    "- **User-Agent Rotation**:\n",
    "  - Created a list of user-agent headers to rotate between requests to avoid being blocked by the server. This helps simulate requests coming from different browsers.\n",
    "\n",
    "### 3. Extracting HTML Data\n",
    "- **Function `reviewsHtml(url, len_page)`**:\n",
    "  - Handles the extraction of HTML content from the review pages.\n",
    "  - Iterates through the specified number of pages, constructs the URL for each page, sends an HTTP GET request, and parses the HTML response using BeautifulSoup.\n",
    "  - Includes error handling to log any failed requests and to print an error message if a page cannot be retrieved.\n",
    "\n",
    "### 4. Parsing Review Data\n",
    "- **Function `getReviews(html_data)`**:\n",
    "  - Takes the parsed HTML data and extracts the required information from each review.\n",
    "  - Navigates through the HTML structure to find the reviewer's name, star rating, review title, review date, and review description.\n",
    "  - Incorporates error handling to ensure that if any piece of data is missing or cannot be parsed, a default value ('N/A') is assigned.\n",
    "\n",
    "### 5. Main Function\n",
    "- **Function `main()`**:\n",
    "  - Orchestrates the entire process.\n",
    "  - Initializes a list of URLs to be scraped and iterates through each URL, calling the `reviewsHtml` function to get the HTML content for the specified number of pages.\n",
    "  - For each page's HTML content, the `getReviews` function is called to extract review data, which is then appended to a list.\n",
    "  - After collecting all reviews, the data is converted into a pandas DataFrame and saved to a CSV file named `amazon_reviews1.csv`.\n",
    "\n",
    "## Execution\n",
    "\n",
    "The script begins by calling the `main()` function, which triggers the scraping process for the specified review pages. Each page is scraped with a slight random delay between requests to avoid detection and blocking. The extracted review data is then compiled into a CSV file, ready for analysis.\n",
    "\n",
    "## Purpose and Benefits\n",
    "\n",
    "The primary purpose of this project is to demonstrate web scraping techniques for collecting user reviews from e-commerce sites. This approach can be extended to various applications, such as sentiment analysis, product feature analysis, and customer feedback evaluation. By automating the data collection process, it saves significant time and effort compared to manual data entry.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project successfully showcases a method to scrape user reviews from Amazon, capturing essential review details and storing them in a structured format for further use. The approach ensures robustness through error handling and user-agent rotation, making it a valuable tool for extracting user-generated content from the web.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3c65efd-ddb8-41a9-b996-1184867fadd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1 from URL: https://www.amazon.com/Waterproof-Weekender-Essentials-Hospital-Overnight/product-reviews/B0CGR1XGVX/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 2 from URL: https://www.amazon.com/Waterproof-Weekender-Essentials-Hospital-Overnight/product-reviews/B0CGR1XGVX/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 3 from URL: https://www.amazon.com/Waterproof-Weekender-Essentials-Hospital-Overnight/product-reviews/B0CGR1XGVX/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 4 from URL: https://www.amazon.com/Waterproof-Weekender-Essentials-Hospital-Overnight/product-reviews/B0CGR1XGVX/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 5 from URL: https://www.amazon.com/Waterproof-Weekender-Essentials-Hospital-Overnight/product-reviews/B0CGR1XGVX/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 6 from URL: https://www.amazon.com/Waterproof-Weekender-Essentials-Hospital-Overnight/product-reviews/B0CGR1XGVX/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 7 from URL: https://www.amazon.com/Waterproof-Weekender-Essentials-Hospital-Overnight/product-reviews/B0CGR1XGVX/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 8 from URL: https://www.amazon.com/Waterproof-Weekender-Essentials-Hospital-Overnight/product-reviews/B0CGR1XGVX/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 9 from URL: https://www.amazon.com/Waterproof-Weekender-Essentials-Hospital-Overnight/product-reviews/B0CGR1XGVX/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 10 from URL: https://www.amazon.com/Waterproof-Weekender-Essentials-Hospital-Overnight/product-reviews/B0CGR1XGVX/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 1 from URL: https://www.amazon.com/LOVEVOOK-Weekender-Compartment-Toiletry-Hospital/product-reviews/B0C58Q4FM1/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 2 from URL: https://www.amazon.com/LOVEVOOK-Weekender-Compartment-Toiletry-Hospital/product-reviews/B0C58Q4FM1/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 3 from URL: https://www.amazon.com/LOVEVOOK-Weekender-Compartment-Toiletry-Hospital/product-reviews/B0C58Q4FM1/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 4 from URL: https://www.amazon.com/LOVEVOOK-Weekender-Compartment-Toiletry-Hospital/product-reviews/B0C58Q4FM1/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 5 from URL: https://www.amazon.com/LOVEVOOK-Weekender-Compartment-Toiletry-Hospital/product-reviews/B0C58Q4FM1/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 6 from URL: https://www.amazon.com/LOVEVOOK-Weekender-Compartment-Toiletry-Hospital/product-reviews/B0C58Q4FM1/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 7 from URL: https://www.amazon.com/LOVEVOOK-Weekender-Compartment-Toiletry-Hospital/product-reviews/B0C58Q4FM1/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 8 from URL: https://www.amazon.com/LOVEVOOK-Weekender-Compartment-Toiletry-Hospital/product-reviews/B0C58Q4FM1/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 9 from URL: https://www.amazon.com/LOVEVOOK-Weekender-Compartment-Toiletry-Hospital/product-reviews/B0C58Q4FM1/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraped page 10 from URL: https://www.amazon.com/LOVEVOOK-Weekender-Compartment-Toiletry-Hospital/product-reviews/B0C58Q4FM1/ref=cm_cr_getr_d_paging_btm_next\n",
      "Scraping completed and data saved to 'amazon_reviews1.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='scraping_errors.log', level=logging.ERROR, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Headers list to rotate user-agents\n",
    "headers_list = [\n",
    "    {\n",
    "        'authority': 'www.amazon.com',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'accept-language': 'en-US,en;q=0.9,bn;q=0.8',\n",
    "        'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "    },\n",
    "    # Add more user-agents here if needed\n",
    "]\n",
    "\n",
    "# Extract Data as HTML object from Amazon review page\n",
    "def reviewsHtml(url, len_page):\n",
    "    soups = []\n",
    "\n",
    "    for page_no in range(1, len_page + 1):\n",
    "        page_url = f\"{url}&pageNumber={page_no}\"\n",
    "        try:\n",
    "            headers = random.choice(headers_list)\n",
    "            response = requests.get(page_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            soups.append(soup)\n",
    "            print(f\"Scraped page {page_no} from URL: {url}\")\n",
    "            time.sleep(random.uniform(2, 5))  # Random delay between 2 and 5 seconds\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Failed to retrieve page {page_no} from URL {page_url}: {e}\")\n",
    "            print(f\"Failed to retrieve page {page_no} from URL {page_url}: {e}\")\n",
    "            break\n",
    "\n",
    "    return soups\n",
    "\n",
    "# Grab reviews name, description, date, stars, title from HTML\n",
    "def getReviews(html_data):\n",
    "    data_dicts = []\n",
    "\n",
    "    boxes = html_data.select('div[data-hook=\"review\"]')\n",
    "\n",
    "    for box in boxes:\n",
    "        try:\n",
    "            name = box.select_one('[class=\"a-profile-name\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            name = 'N/A'\n",
    "\n",
    "        try:\n",
    "            stars = box.select_one('[data-hook=\"review-star-rating\"]').text.strip().split(' out')[0]\n",
    "        except Exception as e:\n",
    "            stars = 'N/A'\n",
    "\n",
    "        try:\n",
    "            title = box.select_one('[data-hook=\"review-title\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            datetime_str = box.select_one('[data-hook=\"review-date\"]').text.strip().split(' on ')[-1]\n",
    "            date = datetime.strptime(datetime_str, '%B %d, %Y').strftime(\"%d/%m/%Y\")\n",
    "        except Exception as e:\n",
    "            date = 'N/A'\n",
    "\n",
    "        try:\n",
    "            description = box.select_one('[data-hook=\"review-body\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            description = 'N/A'\n",
    "\n",
    "        data_dict = {\n",
    "            'Name': name,\n",
    "            'Stars': stars,\n",
    "            'Title': title,\n",
    "            'Date': date,\n",
    "            'Description': description\n",
    "        }\n",
    "\n",
    "        data_dicts.append(data_dict)\n",
    "\n",
    "    return data_dicts\n",
    "\n",
    "# Main Function to run the scraping process\n",
    "def main():\n",
    "    urls = [\n",
    "        \"https://www.amazon.com/Waterproof-Weekender-Essentials-Hospital-Overnight/product-reviews/B0CGR1XGVX/ref=cm_cr_getr_d_paging_btm_next\",\n",
    "        \"https://www.amazon.com/LOVEVOOK-Weekender-Compartment-Toiletry-Hospital/product-reviews/B0C58Q4FM1/ref=cm_cr_getr_d_paging_btm_next\"\n",
    "    ]\n",
    "\n",
    "    all_reviews = []\n",
    "\n",
    "    for url in urls:\n",
    "        soups = reviewsHtml(url, len_page=10)\n",
    "        for soup in soups:\n",
    "            reviews = getReviews(soup)\n",
    "            all_reviews.extend(reviews)\n",
    "\n",
    "    df = pd.DataFrame(all_reviews)\n",
    "    df.to_csv('amazon_reviews1.csv', index=False)\n",
    "    print(\"Scraping completed and data saved to 'amazon_reviews1.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fa7550e-0269-4a03-af1c-19c286f55b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('amazon_reviews1.csv')\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "677f591b-36b6-4bf2-aa97-0334b6f40f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stormy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0 out of 5 stars\\nIt's sturdy, spacious, and...</td>\n",
       "      <td>13/05/2024</td>\n",
       "      <td>I got this bag when it was $14. I honestly was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IrishEyes4Ever</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0 out of 5 stars\\nGreat, for an Affordable T...</td>\n",
       "      <td>16/04/2024</td>\n",
       "      <td>The price is reasonable for what you get. A ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hailey R.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0 out of 5 stars\\nGreat value, great little bag</td>\n",
       "      <td>17/04/2024</td>\n",
       "      <td>I needed an easy weekend duffle for my hospita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ash</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0 out of 5 stars\\nAMAZING! Much better than ...</td>\n",
       "      <td>17/05/2024</td>\n",
       "      <td>This bag is much better than I was expecting w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hannah</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0 out of 5 stars\\nLove it</td>\n",
       "      <td>17/05/2024</td>\n",
       "      <td>Perfect for an overnight bag. Love the size an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Anni</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0 out of 5 stars\\nGood Quality &amp; Stylish</td>\n",
       "      <td>16/03/2024</td>\n",
       "      <td>Bought the bag for my trip to London and it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>andrea.lee</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0 out of 5 stars\\nBeautiful, functional and ...</td>\n",
       "      <td>06/06/2024</td>\n",
       "      <td>I love this bag! I got it for an upcoming trip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Laurie</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0 out of 5 stars\\nLove ytis bag</td>\n",
       "      <td>11/06/2024</td>\n",
       "      <td>This bag is the perfect size for an overnight ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Adrianne D Forrest</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0 out of 5 stars\\nThe Strap Broke</td>\n",
       "      <td>15/06/2024</td>\n",
       "      <td>I bought this bag in January in anticipation o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>momoffive</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0 out of 5 stars\\nStylish and practical</td>\n",
       "      <td>28/06/2024</td>\n",
       "      <td>Beautiful color, style, and bigger than expect...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Name  Stars  \\\n",
       "0                Stormy    5.0   \n",
       "1        IrishEyes4Ever    5.0   \n",
       "2             Hailey R.    5.0   \n",
       "3                   Ash    5.0   \n",
       "4                Hannah    5.0   \n",
       "..                  ...    ...   \n",
       "195                Anni    5.0   \n",
       "196          andrea.lee    5.0   \n",
       "197              Laurie    5.0   \n",
       "198  Adrianne D Forrest    4.0   \n",
       "199           momoffive    5.0   \n",
       "\n",
       "                                                 Title        Date  \\\n",
       "0    5.0 out of 5 stars\\nIt's sturdy, spacious, and...  13/05/2024   \n",
       "1    5.0 out of 5 stars\\nGreat, for an Affordable T...  16/04/2024   \n",
       "2    5.0 out of 5 stars\\nGreat value, great little bag  17/04/2024   \n",
       "3    5.0 out of 5 stars\\nAMAZING! Much better than ...  17/05/2024   \n",
       "4                          5.0 out of 5 stars\\nLove it  17/05/2024   \n",
       "..                                                 ...         ...   \n",
       "195         5.0 out of 5 stars\\nGood Quality & Stylish  16/03/2024   \n",
       "196  5.0 out of 5 stars\\nBeautiful, functional and ...  06/06/2024   \n",
       "197                  5.0 out of 5 stars\\nLove ytis bag  11/06/2024   \n",
       "198                4.0 out of 5 stars\\nThe Strap Broke  15/06/2024   \n",
       "199          5.0 out of 5 stars\\nStylish and practical  28/06/2024   \n",
       "\n",
       "                                           Description  \n",
       "0    I got this bag when it was $14. I honestly was...  \n",
       "1    The price is reasonable for what you get. A ro...  \n",
       "2    I needed an easy weekend duffle for my hospita...  \n",
       "3    This bag is much better than I was expecting w...  \n",
       "4    Perfect for an overnight bag. Love the size an...  \n",
       "..                                                 ...  \n",
       "195  Bought the bag for my trip to London and it is...  \n",
       "196  I love this bag! I got it for an upcoming trip...  \n",
       "197  This bag is the perfect size for an overnight ...  \n",
       "198  I bought this bag in January in anticipation o...  \n",
       "199  Beautiful color, style, and bigger than expect...  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
